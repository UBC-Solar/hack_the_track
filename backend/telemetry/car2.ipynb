{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Getting Data:",
   "id": "adc73fa20db4129b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from os import rename\n",
    "\n",
    "import pandas\n",
    "from sqlalchemy.testing.util import total_size\n",
    "\n",
    "from telemetry import VehicleRaceRecord\n",
    "from telemetry.raw.TelemetryDB import TelemetryDB\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "db = TelemetryDB(\"postgresql+psycopg2://racer:changeme@100.120.36.75:5432/racing\")\n",
    "\n",
    "#Available telemetry signals: ['accx_can', 'accy_can',  'ath', 'gear', 'nmot', 'pbrake_f', 'pbrake_r', 'speed', 'Steering_Angle'\n"
   ],
   "id": "d1adfa664c5282a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#gps data directly from csv file, data is\n",
    "vehicle_id = \"GR86-022-13\"\n",
    "df_gps = pd.read_csv(r\"C:\\Users\\sanar\\PycharmProjects\\hack_the_track\\R2_barber_telemetry_data.csv\")\n",
    "df_gps = df_gps[df_gps['original_vehicle_id'] == vehicle_id]\n",
    "df_lat = df_gps[df_gps['telemetry_name'] == \"VBOX_Lat_Min\"]\n",
    "df_long = df_gps[df_gps['telemetry_name'] == \"VBOX_Long_Minutes\"]\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "4b9d944e29bf9e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_lat['timestamp'] = pd.to_datetime(df_lat['timestamp'])\n",
    "df_long['timestamp'] = pd.to_datetime(df_long['timestamp'])"
   ],
   "id": "808b27e52ee6c1e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_lat.head()",
   "id": "bd325f68595d585"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_lat = df_lat.rename(columns={'telemetry_value': 'value'})\n",
    "df_long = df_long.rename(columns={'telemetry_value': 'value'})\n"
   ],
   "id": "67d4653cf8de5054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# telemetry data for car 2 - GR86-002-000\n",
    "car = db.get_car_race(track=\"barber\", race_number=2, vehicle_code=\"GR86-022-13\")\n",
    "\n",
    "if car:\n",
    "    df_accx = car.get_telemetry(\"accx_can\")\n",
    "    df_accy = car.get_telemetry(\"accy_can\")\n",
    "    df_speed = car.get_telemetry(\"speed\")\n",
    "    df_ath = car.get_telemetry(\"ath\")\n",
    "    df_gear = car.get_telemetry(\"gear\")\n",
    "    df_aps = car.get_telemetry(\"aps\")\n",
    "    df_nmotor = car.get_telemetry(\"nmot\")\n",
    "\n",
    "    df_pbrake_f = car.get_telemetry(\"pbrake_f\")\n",
    "    df_pbrake_r = car.get_telemetry(\"pbrake_r\")\n",
    "    #df_steering = car.get_telemetry(\"Steering_Angle\")\n",
    "\n"
   ],
   "id": "d31e60b3e4b8f430"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_aps.head()",
   "id": "33a24224e4b095fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_ath.head()  #throttle blade position, ignored for now",
   "id": "c60d275a2c131075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_speed.head()",
   "id": "77475ece27bf7570"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#theres some discrepancies- around 1min ish between the gps data and the telemetry",
   "id": "94313b7d99b64d30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#all dataframes from telemetry data\n",
    "\n",
    "\n",
    "all_dfs = [df_accx, df_accy, df_speed, df_aps, df_gear, df_nmotor, df_pbrake_f,\n",
    "           df_pbrake_r]  # note: timestamps have not been modified yet"
   ],
   "id": "7f31be3c706ecc42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "telemetry_values = ['accx_can', 'accy_can', 'gear', 'nmot', 'pbrake_f', 'pbrake_r', 'speed', 'aps']\n",
   "id": "2a32d5bc7e1b50b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_vehicles = [\"GR86-002-000\", \"GR86-004-78\", \"GR86-006-7\", \"GR86-010-16\", \"GR86-013-80\", \"GR86-015-31\", \"GR86-016-55\",\n",
    "                \"GR86-022-13\", \"GR86-025-47\", \"GR86-026-72\", \"GR86-030-18\", \"GR86-033-46\", \"GR86-036-98\", \"GR86-038-93\",\n",
    "                \"GR86-040-3\", \"GR86-047-21\", \"GR86-049-88\", \"GR86-060-2\", \"GR86-063-113\", \"GR86-065-5\"]"
   ],
   "id": "353895e63fb3d258"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#make sure all time stamps start at the same spot\n",
    "for i, df in enumerate(all_dfs):\n",
    "    all_dfs[i] = df.copy()\n",
    "    all_dfs[i]['timestamp'] = pd.to_datetime(all_dfs[i]['timestamp'], unit='ns')\n",
    "\n",
    "start_time = min(df['timestamp'].min() for df in all_dfs)\n",
    "end_time = max(df['timestamp'].max() for df in all_dfs)\n",
    "common_index = pd.date_range(start=start_time, end=end_time, freq='1ms')\n"
   ],
   "id": "5bb2c62abeba3762"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "all relevant functions: indexing, resampling data and combining into dataframes. For now, each car has its own dataframe with corresponding telemetry values\n",
    "note: for now, im ignoring ath (throttle blade position) as the data is missing, but its a minor thing to add later"
   ],
   "id": "df26bee2db90877"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def data_each_car(db, vehicle_id):\n",
    "    # ignoring ath for now\n",
    "    list_all_dfs = []\n",
    "    car = db.get_car_race(track=\"barber\", race_number=2, vehicle_code=vehicle_id)\n",
    "\n",
    "    if car:\n",
    "        df_accx = car.get_telemetry(\"accx_can\")\n",
    "        df_accy = car.get_telemetry(\"accy_can\")\n",
    "        df_speed = car.get_telemetry(\"speed\")\n",
    "        df_ath = car.get_telemetry(\"ath\")\n",
    "        df_gear = car.get_telemetry(\"gear\")\n",
    "        df_aps = car.get_telemetry(\"aps\")\n",
    "        df_nmotor = car.get_telemetry(\"nmot\")\n",
    "\n",
    "        df_pbrake_f = car.get_telemetry(\"pbrake_f\")\n",
    "        df_pbrake_r = car.get_telemetry(\"pbrake_r\")\n",
    "        list_all_dfs = [df_accx, df_accy, df_speed, df_gear, df_aps, df_nmotor, df_pbrake_f, df_pbrake_r]\n",
    "    return list_all_dfs\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "227f0abc3c715543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd06ccf32fd38ccd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "db = TelemetryDB(\"postgresql+psycopg2://racer:changeme@100.120.36.75:5432/racing\")\n",
    "\n",
    "all_car2_dfs = data_each_car(db, \"GR86-022-13\")"
   ],
   "id": "ce497cfe06b4fb9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#gets common index, ensures timestamps are in datetime format.\n",
    "\n",
    "def index(list_dfs):\n",
    "    for i, df in enumerate(list_dfs):\n",
    "        list_dfs[i] = df.copy()\n",
    "        list_dfs[i]['timestamp'] = pd.to_datetime(list_dfs[i]['timestamp'], unit='ns')\n",
    "        if 'telemetry_value' in list_dfs[i].columns:\n",
    "            list_dfs[i].rename(columns={'telemetry_value': 'value'}, inplace=True)\n",
    "\n",
    "    start_time = min(df['timestamp'].min() for df in list_dfs)\n",
    "    end_time = max(df['timestamp'].max() for df in list_dfs)\n",
    "    common_index = pd.date_range(start=start_time, end=end_time, freq='1ms')\n",
    "    return common_index, list_dfs\n"
   ],
   "id": "f5fc0342b5a9a5ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_index, dfs = index(all_dfs)",
   "id": "425271d492eb7f29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "#resample and interpolate data\n",
    "def resample(df, common_index):\n",
    "    df_resampled = df.copy()\n",
    "    df = df[~df['timestamp'].duplicated()]\n",
    "    df_new = df.set_index('timestamp', inplace=False)\n",
    "    df_resampled['value'] = pd.to_numeric(df_resampled['value'], errors='coerce')\n",
    "\n",
    "    df_resampled = df_new.reindex(common_index).interpolate(\n",
    "        method='time')  #timeâ€™: Works on daily and higher resolution data to interpolate given length of interval.\n",
    "    df_resampled['value'] = df_resampled['value'].ffill().bfill()\n",
    "    df_resampled.drop(columns=['name'], inplace=True, errors='ignore')\n",
    "\n",
    "    return df_resampled"
   ],
   "id": "66e9d6be73b895ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#resampling seems to give some NaN values for some fields, so use forward/backward fill to take care of that",
   "id": "47b6ad5e3d761367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f9b48499e56fbbf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_car2_dfs[1].head()",
   "id": "c8510115926824ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_gear.tail()",
   "id": "f888d442d137c734"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "telemetry_names = ['accx', 'speed', 'accy', 'pbrake_f', 'nmot', 'gear', 'pbrake_r', 'aps']\n",
    "\n",
    "\n",
    "def combine_dfs_car(telemetry_names, common_index, all_dfs):\n",
    "    combined_df = pd.DataFrame(index=common_index)\n",
    "\n",
    "    for name, df in zip(telemetry_names, all_dfs):\n",
    "        df_interp = resample(df, common_index)\n",
    "        combined_df[name] = pd.to_numeric(df_interp['value'], errors='coerce').values\n",
    "\n",
    "    return combined_df\n"
   ],
   "id": "c5bb910d56ce9004"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "combine_dfs_car(telemetry_names, common_index, all_dfs).head()",
   "id": "2b65ffe737205a0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data collection for all cars",
   "id": "1aa94dcb9ac1f54a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "db = TelemetryDB(\"postgresql+psycopg2://racer:changeme@100.120.36.75:5432/racing\")",
   "id": "ae9d7f8181051293"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "telemetry_names = ['accx', 'accy', 'gear', 'nmot', 'aps', 'pbrake_f', 'pbrake_r', 'Longitude', 'Latitude']",
   "id": "40c3f4365819c34b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_gps = pd.read_csv(r\"C:\\Users\\sanar\\PycharmProjects\\hack_the_track\\R2_barber_telemetry_data.csv\")\n",
    "df_gps = df_gps[df_gps['original_vehicle_id'] == \"GR86-002-000\"]\n",
    "df_lat = df_gps[df_gps['telemetry_name'] == \"VBOX_Lat_Min\"]\n",
    "df_long = df_gps[df_gps['telemetry_name'] == \"VBOX_Long_Minutes\"]\n",
    "\n"
   ],
   "id": "7efb0f6fadaaa51b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_lat = df_lat.rename(columns={'telemetry_value': 'value'})\n",
    "df_long = df_long.rename(columns={'telemetry_value': 'value'})\n"
   ],
   "id": "4bd427583c06d08b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_long.head()  #theres two fields for vehicle if, im sticking with vehicle id",
   "id": "f7a2d8d72cb0d22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#all dataframes from telemetry data\n",
    "\n",
    "\n",
    "all_dfs_lat = [df_accx, df_accy, df_speed, df_aps, df_gear, df_nmotor, df_pbrake_f,\n",
    "               df_pbrake_r, df_lat, df_long]  # note: timestamps have not been modified yet\n",
    "common_index, all_dfs_l = index(all_dfs_lat)"
   ],
   "id": "372347cfdd63b65f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_dfs_l[9].head()",
   "id": "9f2179b6f6083419"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "temp_lon = resample(all_dfs_lat[9], common_index)\n",
    "temp_lat = resample(all_dfs_lat[8], common_index)"
   ],
   "id": "dbd4c6568541fcfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "temp_lon.head()",
   "id": "c5d8998db8d6a0e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.plot(temp_lat['value'], temp_lon['value'])",
   "id": "fbb8d698bf557d66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "telemetry_names = ['accx', 'accy', 'speed', 'gear', 'aps', 'nmot', 'pbrake_f', 'pbrake_r', 'latitude', 'longitude']\n",
   "id": "86c5c86f8d34675e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_dfs_all_cars = []\n",
    "for vehicle in all_vehicles:\n",
    "    all_dfs_car = data_each_car(db, vehicle_id)\n",
    "    df_lat_temp = df_lat[df_lat['vehicle_id'] == vehicle_id]\n",
    "    df_long_temp = df_long[df_long['vehicle_id'] == vehicle_id]\n",
    "\n",
    "    all_dfs_car.append(df_lat_temp)\n",
    "    all_dfs_car.append(df_long_temp)\n",
    "\n",
    "    #get common index\n",
    "    my_index, list_dfs = index(all_dfs_car)\n",
    "\n",
    "    combined_df_car = combine_dfs_car(telemetry_names, my_index, list_dfs)\n",
    "    final_dfs_all_cars.append(combined_df_car)\n",
    "    #now interpolate, adn get combined df"
   ],
   "id": "abe9d36dbc2c97c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#takes like 4 minutes for all data to load, idk if thats bad??",
   "id": "e6088a4b87653a56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_dfs_all_cars[0].head()",
   "id": "8aab435d12d4d572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sample_df = final_dfs_all_cars[7]",
   "id": "8652505dc956dbdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#plt.plot(sample_df, sample_df['Latitude'])\n",
    "plt.plot(sample_df['latitude'], sample_df['longitude'], color='red')\n"
   ],
   "id": "9e9ce8e95d9e083"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_dfs_all_cars[7].tail()",
   "id": "aa7b8d68f01dfedd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#why doesn it work for gear/pbrake_rear/nmot?? - fixed, was due to interpolation not filling NaN values",
   "id": "752c8e899292d00a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#im ignoring all above warnings for now",
   "id": "c0af7f975fe2161e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install torch",
   "id": "59e626739d313250"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ],
   "id": "f4cfbd1664c0e835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "id": "5a8a50e041b72d5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "split into training/testin data\n",
   "id": "61afde75e698b511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#right now, all the data is stored in final_dfs_all_cars\n",
    "import sklearn"
   ],
   "id": "a6a9a5978f0491bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "iniitally, lets just use individual cars for training/testing instead of splitting\n",
    "inputs to the neural network include : state, nmotor, ath, accx, accy, longitude, latitude\n",
    "\n",
    "was initally supposed to include ath and laptrigger_lapdist but theres no data for them currently\n",
    "\n",
    "#y avlues (control inputs required): pbrake_f, pbrake_r, gear, aps"
   ],
   "id": "6c6dcdaa33adc86e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ],
   "id": "29abbbdd765d9074"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(telemetry_names)",
   "id": "d397d68f046ed081"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#define state inputs:\n",
    "\n",
    "input_state = ['accx', 'accy', 'speed', 'nmot', 'latitude', 'longitude']\n",
    "output_control = ['gear', 'aps', 'pbrake_f', 'pbrake_r']\n",
    "\n",
    "#each df - convert to tensors - tensor dataset - dataloader - feed to NN\n"
   ],
   "id": "711cf3078230253f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_df = pd.concat(final_dfs_all_cars)\n",
   "id": "c7c824b14e04dbf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_df.head()",
   "id": "5f5c19584aa68251"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "datasets = []  # one dataset per car\n",
    "\n",
    "for df in final_dfs_all_cars:  # list of dataframes, one per car\n",
    "    x_values = torch.tensor(df[input_state].values, dtype=torch.float32)\n",
    "    y_values = torch.tensor(df[output_control].values, dtype=torch.float32)\n",
    "    datasets.append(TensorDataset(x_values, y_values))\n"
   ],
   "id": "ad00f5cf01099787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Flatten training data\n",
    "X_train = torch.cat([torch.tensor(df[input_state].values, dtype=torch.float32)\n",
    "                     for df in final_dfs_all_cars[:-3]], dim=0)\n",
    "Y_train = torch.cat([torch.tensor(df[output_control].values, dtype=torch.float32)\n",
    "                     for df in final_dfs_all_cars[:-3]], dim=0)\n",
    "\n",
    "# Flatten testing data\n",
    "X_test = torch.cat([torch.tensor(df[input_state].values, dtype=torch.float32)\n",
    "                    for df in final_dfs_all_cars[-3:]], dim=0)\n",
    "Y_test = torch.cat([torch.tensor(df[output_control].values, dtype=torch.float32)\n",
    "                    for df in final_dfs_all_cars[-3:]], dim=0)\n"
   ],
   "id": "331e6b344269bf43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sequence_length = 100\n",
    "input_size = len(input_state)\n",
    "output_size = len(output_control)\n",
    "\n",
    "# Training\n",
    "num_train_seq = X_train.shape[0] // sequence_length\n",
    "X_train = X_train[:num_train_seq * sequence_length]\n",
    "Y_train = Y_train[:num_train_seq * sequence_length]\n",
    "\n",
    "# Testing\n",
    "num_test_seq = X_test.shape[0] // sequence_length\n",
    "X_test = X_test[:num_test_seq * sequence_length]\n",
    "Y_test = Y_test[:num_test_seq * sequence_length]\n"
   ],
   "id": "f9bb0d0c1705ca68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n"
   ],
   "id": "933ddda4e03ca9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "59fee6b0ec559d09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# training_data = datasets[:-3]\n",
    "# testing_data = datasets[-3:]"
   ],
   "id": "45c7caa5344617d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# #combining training sets into a single dataset\n",
    "#\n",
    "# from torch.utils.data import ConcatDataset\n",
    "#\n",
    "# training_data = ConcatDataset(training_data)\n",
    "# testing_data = ConcatDataset(testing_data)\n"
   ],
   "id": "3ad9a2c699b16c4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "now that splitting of data is complete, build a basic neural network, minimal layers",
   "id": "fb379b2fe8d7b88b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, seq_length, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size  #dim of memory inside lstm\n",
    "        self.num_layers = num_layers  #stacked lstm layers\n",
    "        #lstm: long short term memory - looks at lng term dependencies in sequential data\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  #correspond to input data shape\n",
    "        self.seq_length = seq_length  #no of timestamps to look at to predict the next control output\n",
    "\n",
    "        #num classes is the no of outputs predicted by the model\n",
    "\n",
    "        #to convert memory vector to outputs (shaping constraints)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #inital hidden, cell states - these are internal memory vectors\n",
    "        #hidden = short term memory, current output of LSTM at a given time\n",
    "        hidden_state = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        #cell state = long term memory, stores trends (remmebers info over many time steps)\n",
    "\n",
    "        cell_states = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        #forward propagate lstm\n",
    "        out, _ = self.lstm(x, (hidden_state,\n",
    "                               cell_states))  #out; tensor of shape(batch_soze, seq_length, hidden_size) - at the final time step\n",
    "        #decode the hidden state of t\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "id": "a24c8f09d8d1b322"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "len(input_state)\n",
    "len(output_control)"
   ],
   "id": "63da14c37d9da2f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "len(train_dataset)\n",
    "input_size = 6\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "seq_length = 100\n",
    "num_classes = 4"
   ],
   "id": "91cfe9a9bad20f7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = RNN(input_size=6, hidden_size=32, num_layers=1, seq_length=100, num_classes=4).to(device)",
   "id": "f3bff86af5f121f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  #lr - learnign rate, optimiser is Adam"
   ],
   "id": "c7b2c0c054c8d18f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#training the model\n",
    "num_epochs = 100\n",
    "total_step = len(train_loader)  #total number of steps is the length of the trainign dataest\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_state, output_control) in enumerate(train_loader):  #i = batch index, input, output data\n",
    "        #input_state = input_state.reshape(-1, seq_length, input_size ).to(device)\n",
    "        input_state = input_state.unsqueeze(0).to(device)\n",
    "        output_control = output_control[-1,:].unsqueeze(0).to(device) #one output per input sequence (row of num_classes size, so reshapingn is not required)\n",
    "\n",
    "        #forward pass:\n",
    "        outputs = model(input_state) #initial pass through NN\n",
    "        loss = criterion(outputs, output_control) #computes error\n",
    "\n",
    "        #backward and optimise\n",
    "        optimizer.zero_grad()  #set gradients to zero, so previous batches dont overlap\n",
    "        loss.backward() #calculates gradients for all parameters to update weights\n",
    "        optimizer.step() #optimally, updates each parameter in the direction that reduces the loss\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for state, control in test_loader:\n",
    "            state = state.reshape(-1, seq_length,input_size).to(device)\n",
    "           # state = state.to(device)\n",
    "            control = control.to(device)\n",
    "            outputs = model(state)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += control.size(0)\n",
    "            correct += (predicted==control).sum().item()\n",
    "\n",
    "        print('Test Accuracy: {:.4f}'.format(100 * correct / total))\n",
    "\n",
    "    torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2615d390ecc195fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training the model\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_step = len(train_loader)  # steps per epoch\n",
    "    for i, (input_state, output_control) in enumerate(train_loader):\n",
    "        # input_state: (seq_length, input_size)\n",
    "        # output_control: (seq_length, output_size)\n",
    "\n",
    "        # Add batch dimension: (1, seq_length, input_size)\n",
    "        input_state = input_state.unsqueeze(0).to(device)\n",
    "\n",
    "        # Many-to-one: take only last timestep as target\n",
    "        output_control = output_control[-1, :].unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_state)\n",
    "        loss = criterion(outputs, output_control)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:  # print more frequently since steps are smaller\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for state, control in test_loader:\n",
    "            # Add batch dimension\n",
    "            state = state.unsqueeze(0).to(device)\n",
    "            target = control[-1, :].unsqueeze(0).to(device)\n",
    "\n",
    "            outputs = model(state)\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] Test Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), f'model_epoch{epoch+1}.pt')\n"
   ],
   "id": "a281af04bd019fe2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "graphing:\n"
   ],
   "id": "f5ab635299e3dd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#all vehicles, all laps\n",
    "plt.scatter(df_lat_interp['telemetry_value'], df_long_interp['telemetry_value'], color='magenta')\n",
    "\n",
    "plt.show()"
   ],
   "id": "7f6ab5c3bfa53046"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e2d0251554175085"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we (mostly) have the data for the neural network in the form of a dataframe for each car, lets build the neural network",
   "id": "c64cf45637f35d57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1b4ec1b0e08c44ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "misc stuff to be ignored till i figure them out :",
   "id": "f1bb8cf708b68c5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax1 = ax.twinx()\n",
    "\n",
    "plt.plot(df_speed_in['index'], df_speed_in['value'], color='blue')\n",
    "plt.plot(df_speed['timestamp'], df_speed_in['value'], color='green')\n",
    "plt.xticks(rotation=90)\n",
    "#plt.plot(df_speed['value'], label=\"ax\")"
   ],
   "id": "ba84338425ad7a58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#can ignore everything after:",
   "id": "194459569cc4c01e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def data_all_cars(db, track, race_number, vehicle_codes, telemetry_values):\n",
    "    all_resampled = []  # store each signal dataframe aligned by timestamp\n",
    "\n",
    "    # Step 1: Collect all raw data first\n",
    "    raw_dfs = []\n",
    "    for vehicle in vehicle_codes:\n",
    "        car = db.get_car_race(track=track, race_number=race_number, vehicle_code=vehicle)\n",
    "        if car is None:\n",
    "            print(f\"Warning: No data for {vehicle}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        for sig in telemetry_values:\n",
    "            df = car.get_telemetry(sig)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            # df = df[['timestamp', 'value']].rename(columns={'value': f\"{vehicle}_{sig}\"})\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ns')\n",
    "            raw_dfs.append(df)\n",
    "\n",
    "    if not raw_dfs:\n",
    "        return None\n",
    "\n",
    "    # Step 2: Compute global overlapping window ONCE\n",
    "    start_time = max(df['timestamp'].min() for df in raw_dfs)\n",
    "    end_time = min(df['timestamp'].max() for df in raw_dfs)\n",
    "\n",
    "    common_index = pd.date_range(start=start_time, end=end_time, freq='1ms')\n",
    "\n",
    "    # Step 3: Resample each signal ONCE and index by timestamp\n",
    "    for df in raw_dfs:\n",
    "        df_r = resample(df, common_index)\n",
    "        all_resampled.append(df_r)\n",
    "\n",
    "    # # Step 4: Fast combine using join\n",
    "    # df_all = all_resampled[0]\n",
    "    # for df in all_resampled[1:]:\n",
    "    #     df_all = df_all.join(df['index'])\n",
    "    #\n",
    "    # df_all = df_all.reset_index()\n",
    "    return all_resampled\n"
   ],
   "id": "1aa07cb622c09e79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_data(all_vehicles, db):\n",
    "    list_dfs = []\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    for vehicle_id in all_vehicles:\n",
    "        car = db.get_car_race(track=\"barber\", race_number=2, vehicle_code=vehicle_id)\n",
    "        if car:\n",
    "            new_df['accx_can'] = car.get_telemetry(\"accx_can\")['value']\n",
    "            new_df['accy_can'] = car.get_telemetry(\"accy_can\")['value']\n",
    "            new_df['speed'] = car.get_telemetry(\"speed\")['value']\n",
    "            new_df['ath'] = car.get_telemetry(\"ath\")['value']\n",
    "            new_df['gear'] = car.get_telemetry(\"gear\")['value']\n",
    "            new_df['pbrake_f'] = car.get_telemetry(\"pbrake_f\")['value']\n",
    "            new_df['pbrake_r'] = car.get_telemetry(\"pbrake_r\")['value']\n",
    "            new_df['aps'] = car.get_telemetry(\"aps\")['value']\n",
    "            new_df['nmotor'] = car.get_telemetry(\"nmot\")['value']\n",
    "            list_dfs.append(df)\n",
    "\n",
    "    # concatenate all vehicles\n",
    "    combined_df = pd.concat(list_dfs, ignore_index=True)\n",
    "\n",
    "    return combined_df\n"
   ],
   "id": "b5840ccd36848bf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aeb06fd55fc9bf12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_df = get_data(all_vehicles, db)",
   "id": "90528fc488977118"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_data(car):\n",
    "    for (vehicle_id in all_vehicles):\n",
    "        car = db.get_car_race(track=\"barber\", race_number=2, vehicle_code=vehicle_id)\n",
    "        if car:\n",
    "            df_accx = car.get_telemetry(\"accx_can\")\n",
    "            df_accy = car.get_telemetry(\"accy_can\")\n",
    "            df_speed = car.get_telemetry(\"speed\")\n",
    "            df_ath = car.get_telemetry(\"ath\")\n",
    "            df_gear = car.get_telemetry(\"gear\")\n",
    "            df_aps = car.get_telemetry(\"aps\")\n",
    "            df_nmotor = car.get_telemetry(\"nmot\")\n",
    "\n",
    "            df_pbrake_f = car.get_telemetry(\"pbrake_f\")\n",
    "            df_pbrake_r = car.get_telemetry(\"pbrake_r\")\n",
    "\n",
    "\n"
   ],
   "id": "6cc54095f9a92b63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e839a25dd07dd2d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#interpolate data before feeding into neural network to account for any missing data, and smoothing out time samples\n",
    "\n"
   ],
   "id": "3b6a76061e4df391"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def median(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    time_diff = df['timestamp'].diff().dt.total_seconds()\n",
    "    med = time_diff.median()\n",
    "    if pd.isna(med) or med == 0:\n",
    "        med = 0.001  # default 1 ms\n",
    "    return med\n",
    "\n",
    "\n",
    "def interp(df):\n",
    "    df_in = df.copy()\n",
    "    df_in['timestamp'] = pd.to_datetime(df_in['timestamp'])\n",
    "    df_in = df_in.set_index('timestamp').sort_index()\n",
    "\n",
    "    # remove duplicate timestamps\n",
    "    df_in = df_in[~df_in.index.duplicated()]\n",
    "\n",
    "    # df['timestamp'] = pd.to_datetime(df['timestamp'] for df in all_dfs)\n",
    "\n",
    "    # define a global min and max timestamp across all cars\n",
    "    global_start = min(df['timestamp'].min() for df in all_dfs)\n",
    "    global_end = max(df['timestamp'].max() for df in all_dfs)\n",
    "\n",
    "    common_index = pd.date_range(start=global_start, end=global_end)\n",
    "\n",
    "    # ensure at least one numeric column exists\n",
    "    numeric_cols = df_in.select_dtypes(include='number').columns\n",
    "    if len(numeric_cols) == 0:\n",
    "        raise ValueError(\"No numeric columns to interpolate.\")\n",
    "\n",
    "    # median interval in seconds\n",
    "    sampling_freq_s = median(df_in.reset_index())\n",
    "    sampling_freq_ms = max(1, int(round(sampling_freq_s * 1000)))\n",
    "    freq = f'{sampling_freq_ms}ms'\n",
    "\n",
    "    # reindex and interpolate only numeric columns\n",
    "    # df_resampled = df_in[numeric_cols].reindex(new_index).interpolate(method='linear')\n",
    "    df_resampled = df_in[numeric_cols].reindex(common_index).interpolate(method='linear')\n",
    "\n",
    "    # optional: keep original index as a column\n",
    "    df_resampled = df_resampled.rename_axis('timestamp').reset_index()\n",
    "\n",
    "    return df_resampled\n"
   ],
   "id": "1043d413a2d103fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f9cc555932667de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e64589809e05b14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad0c48a623f0b320"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bef7815b896e817b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "55c76255791c086"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d02c071886cd92b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(df_accx_in['index'], df_accx_in['value'], color='blue')\n",
    "#plt.plot(df_accy_in['index'], df_accy_in['value'], color='green')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"AccX, AccY\")"
   ],
   "id": "63dcfe2b7b28c7a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(df_accx['timestamp'], df_accx['value'], color='blue')\n",
    "#plt.plot(df_accy_in['index'], df_accy_in['value'], color='green')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"AccX, AccY\")"
   ],
   "id": "27451221a8439ad3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8d5696545d77f55f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8935fcce2adf9569"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def checkNan(df):\n",
    "    any_nan = df_accx.isna().any().any()\n",
    "    return any_nan"
   ],
   "id": "e74033dfe4047a2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "print(checkNan(df) for df in all_dfs)"
   ],
   "id": "a97536ee58ec0df7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.plot(df_accx_in['timestamp'], df_accx_in['value'], color='red')",
   "id": "586536e9008f1bbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.plot(df_accx_res['value'])",
   "id": "59d6f3ab535d3b33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "id": "e9a48470888a18bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f6443c99aa433a8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i].index = pd.to_datetime(dfs[i].index)\n",
    "\n",
    "start = max(df.index.min() for df in dfs)  # latest start time across all\n",
    "end = min(df.index.max() for df in dfs)  # earliest end time across all\n",
    "\n",
    "# 3) Choose your sampling interval (example: 50 ms)\n",
    "freq = \"5ns\"\n",
    "\n",
    "# 4) Build the common time axis\n",
    "time_index = pd.date_range(start=start, end=end, freq=freq)\n",
    "\n",
    "# 5) Resample + interpolate each dataframe to the common index\n",
    "aligned = []\n",
    "for df in dfs:\n",
    "    aligned_df = df.reindex(time_index).interpolate(method=\"linear\")\n",
    "    aligned.append(aligned_df)\n",
    "\n"
   ],
   "id": "b203d8ce6e095aa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1af42460e937a0ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "len(df_accx)\n",
    "len(df_speed)"
   ],
   "id": "fde93dc60dbdc72b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_accx.head()",
   "id": "c185853329a1e521"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "364fc6ea23c6c1b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
